{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e09bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiago/miniconda3/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nannyml as nml\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93b4ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment set up\n",
    "dataset = 'avocados'\n",
    "min_n_train = 52\n",
    "n_test = 12\n",
    "n_prod = 24\n",
    "n_simulations = 3000\n",
    "n_retrainings = 3\n",
    "freq = 'W'\n",
    "models = ['LGBMRegressor', 'ElasticNet', 'RandomForestRegressor', 'MLPRegressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3546632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_comparison = pd.read_parquet(f'../results/performance_estimation/{dataset}/pe_comparison_{dataset}_{models[0]}_{n_simulations}_simulations_{n_prod}_prod_.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05a6dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "degradation_threshold = 0.15\n",
    "pe_comparison['upper_threshold'] = degradation_threshold\n",
    "pe_comparison['estimated_alert'] = np.where(pe_comparison['value'] > degradation_threshold, True, False)\n",
    "pe_comparison['realized_alert'] = np.where(pe_comparison['realized'] > degradation_threshold, True, False)\n",
    "\n",
    "pe_comparison = pe_comparison[['realized', 'value', 'upper_threshold', 'estimated_alert', 'realized_alert', 'simulation_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e28ea6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized</th>\n",
       "      <th>value</th>\n",
       "      <th>upper_threshold</th>\n",
       "      <th>estimated_alert</th>\n",
       "      <th>realized_alert</th>\n",
       "      <th>simulation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029868</td>\n",
       "      <td>0.133711</td>\n",
       "      <td>0.15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118612</td>\n",
       "      <td>0.133711</td>\n",
       "      <td>0.15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071403</td>\n",
       "      <td>0.133711</td>\n",
       "      <td>0.15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036387</td>\n",
       "      <td>0.133711</td>\n",
       "      <td>0.15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067065</td>\n",
       "      <td>0.133711</td>\n",
       "      <td>0.15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   realized     value  upper_threshold  estimated_alert  realized_alert  \\\n",
       "0  0.029868  0.133711             0.15            False           False   \n",
       "1  0.118612  0.133711             0.15            False           False   \n",
       "2  0.071403  0.133711             0.15            False           False   \n",
       "3  0.036387  0.133711             0.15            False           False   \n",
       "4  0.067065  0.133711             0.15            False           False   \n",
       "\n",
       "   simulation_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e435bf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    15521\n",
       "True      3285\n",
       "Name: realized_alert, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_comparison['realized_alert'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16190649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    17465\n",
       "True      1341\n",
       "Name: estimated_alert, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_comparison['estimated_alert'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4248d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945 out of 3000 models (64.8 %) degradaded\n",
      "NannyML estimated at least a degradation in 350 out of the 1945 (18.0 %) models that degradated\n",
      "There were 3285 degradations alerts\n",
      "NannyML estimated 394 out of 3285 (12.0 %) of the degradation alerts correctly\n",
      "NannyML estimated 14968 (79.6 %) of the time the behaivoir of the performance correctly\n"
     ]
    }
   ],
   "source": [
    "positive_alerts = pe_comparison[pe_comparison['realized_alert'] == True]\n",
    "negative_alerts = pe_comparison[pe_comparison['realized_alert'] == False]\n",
    "\n",
    "tp_alerts = sum(positive_alerts['estimated_alert'] == positive_alerts['realized_alert'])\n",
    "tn_alerts = sum(negative_alerts['estimated_alert'] == negative_alerts['realized_alert'])\n",
    "\n",
    "# degradations_per_model = pe_comparison.groupby(['simulation_id'])[['realized_alert']].sum().reset_index()\n",
    "# num_degradated_models = len(degradations_per_model[degradations_per_model['realized_alert'] > 5])\n",
    "\n",
    "# print(f\"{num_degradated_models} out of {len(degradations_per_model)} models ({np.round(100 * num_degradated_models / len(degradations_per_model), 1)} %) degradaded\")\n",
    "\n",
    "degradations_per_model = pe_comparison.groupby(['simulation_id'])[['realized_alert', 'estimated_alert']].sum().reset_index()\n",
    "num_degradated_models = len(degradations_per_model[degradations_per_model['realized_alert'] > 0])\n",
    "\n",
    "degradations_per_model['is_true_positive'] = np.where((degradations_per_model['realized_alert'] > 0) & (degradations_per_model['estimated_alert'] > 0), True, False)\n",
    "tp_alerts_per_model = degradations_per_model[degradations_per_model['is_true_positive'] == True]\n",
    "\n",
    "print(f\"{num_degradated_models} out of {len(degradations_per_model)} models ({np.round(100 * num_degradated_models / len(degradations_per_model), 1)} %) degradaded\")\n",
    "print(f\"NannyML estimated at least a degradation in {len(tp_alerts_per_model)} out of the {num_degradated_models} ({np.round(100 * len(tp_alerts_per_model) / num_degradated_models, 1)} %) models that degradated\")\n",
    "print(f\"There were {len(positive_alerts)} degradations alerts\")\n",
    "print(f\"NannyML estimated {tp_alerts} out of {len(positive_alerts)} ({np.round(tp_alerts/len(positive_alerts) * 100, 1)} %) of the degradation alerts correctly\")\n",
    "print(f\"NannyML estimated {tp_alerts + tn_alerts} ({np.round((tp_alerts + tn_alerts) / len(pe_comparison) * 100, 1)} %) of the time the behaivoir of the performance correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb93fd7",
   "metadata": {},
   "source": [
    "1945 out of 3000 models (64.8 %) degradaded\n",
    "NannyML estimated at least a degradation in 350 out of the 1945 (18.0 %) models that degradated\n",
    "There were 3285 degradations alerts\n",
    "NannyML estimated 394 out of 3285 (12.0 %) of the degradation alerts correctly\n",
    "NannyML estimated 14968 (79.6 %) of the time the behaivoir of the performance correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
